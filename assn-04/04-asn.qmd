---
title: "CSCI/DASC 6020: Written Assignment 04"
author: "Robert Johnson"
date: "`r format(Sys.time(), '%d %B %Y')`"
number-sections: true
number-depth: 3
format:
  html:
    toc: true
    toc-location: right
    number-sections: true
    number-depth: 3
    html-math-method: katex
    embed-resources: true
    code-fold: true
# bibliography: dasc-6000.bib 
# csl: ieee-with-url.csl
# linkcolor: red
# urlcolor: blue
# link-citations: yes
# header-includes:
#   - \usepackage[ruled,vlined,linesnumbered]{algorithm2e}
---

```{r setup, include=FALSE} 
knitr::opts_chunk$set(warning = FALSE, message = FALSE) 
```

# Assignment Goal {.unnumbered}

The goal of this assignment is to demonstrate your understanding of how the concepts from information theory can be used to build machine learning models for predictive tasks. 



# Calculating Entropy

The figure below shows a set of eight Scrabble pieces.

![](./scrabble.png)

1. What is the entropy (in bits) of the letters in this set?
    
```{r}
entropy <- function(p)
  -sum(p * log2(p))
```
    
```{r}
# discrete probabilities
p <- c(3/8, 1/8, 1/8, 1/8, 1/8, 1/8)
# calculate entropy
e <- entropy(p)
e
```
    
1. What would be the reduction in entropy (i.e., the information gain) in bits if we split these letters into two sets, one containing the vowels and the other containing the consonants?

Splitting gives us the following sets (i'm considering y as a vowel given the pronunciation):
    
```{r}
vow <- c('O','Y','O','O')
con <- c('X','M','R','N')
print(vow)
print(con)
```
    
Calculating the entropy for our set of vowels give us:    
    
```{r}
p1 <- c(3/4, 1/4)
e1 <- entropy(p1)
print(e1)
```
    
Calculating entropy for the consonants gives us:
    
```{r}
p2 <- c(1/4, 1/4, 1/4, 1/4)
e2 <- entropy(p2)
print(e2)
```
    
Weighting the two and combining leaves us with the following entropy:    
    
```{r}
e3 <- (1/2 * e2) + (1/2 * e1)
print(e3)
```

So we have reduced our entropy by the following:

```{r}
print(e - e3)
```

    
1. What is the maximum possible entropy (in bits) for a set of eight Scrabble pieces?
    
    If all probabilties are equally likely then the entropy is:
    
```{r}
p <- c(rep(1/8, 8))
entropy(p)
```
    
1. In general, which is preferable when you are playing Scrabble: a set of letters with high entropy, or a set of letters with low entropy?
    
    High entropy so you can make more words
    
# Decision Tree Construction

A convicted criminal who reoffends after release is known as a recidivist. The Table below lists a dataset that describes prisoners released on parole, and whether they reoffended within two years of release.

```{r}
#| echo: false
c1 <- c(1,2,3,4,5,6)
c2 <- c("false", "false", "false", "true", "true", "true")
c3 <- c("true", "false", "true", "false", "false", "false")
c4 <- c("false", "false", "false", "false", "true", "false")
c5 <- c("true", "false", "true", "false", "true", "false")
df1 <- data.frame(c1, c2, c3, c4, c5)
#print(df1)
```


```{r}
#| echo: false
#| tbl-cap-location: margin
knitr::kable(df1, caption = 'Prisoners on parole.', col.names = c('ID', 'Good Behavior', 'Age Below 30', 'Drug Dependent', 'Recidivist'), align = "lrrrr")
```



This dataset lists six instances where prisoners were granted parole. Each of these instances are described in terms of three binary descriptive features (**Good Behavior**, **Age Below 30**, **Drug Dependent**) and a binary target feature, **Recidivist**. The **Good Behavior** feature has a value of **true** if the prisoner had not committed any infringements during incarceration, the **Age Below 30** has a value of **true** if the prisoner was under 30 years of age when granted parole, and the **Drug Dependent** feature is **true** if the prisoner had a drug addiction at the time of parole. The target feature, **Recidivist**, has a **true** value if the prisoner was arrested within two years of being released; otherwise it has a value of **false**.


1. Using this dataset, construct the decision tree that would be generated by the ID3 algorithm, using entropy-based information gain.
    
    ID3 Works by selecting the feature that provides the most information gain (biggest reduction in         entropy in the target feature) at each step of construction. Lets first compute the entropy of the       target feature
    
```{r}
p <- c(1/2,1/2)
e_target <- entropy(p)
print(paste0("Dataset entropy: ", e_target))
```

Next the Information gain for each column would be computed to do this the independent features are broken down and the entropy of the target feature is computed for each option in the feature independently
    
```{r}
#Entropy for good behavior = false
p <- c(2/3,1/3)
e_false <- entropy(p)
#print(paste0("P(Recidivist|Good Behavior = false) = ", 1/2))
#print(paste0("Entropy(Recidivist|Good Behavior=false) = ", e_false))

#Entropy for good behavior = true
p <- c(1/3,2/3)
e_true <- entropy(p)
#print(paste0("P(Recidivist|Good Behavior = true) = ", 1/2))
#print(paste0("Entropy(Recidivist|Good Behavior=true) = ", e_true))

#Information gain for feature good behavior
ig <- e_target - ((1/2*e_false) + (1/2*e_true))
print(paste0("Information gain for feature Good Behavior: ", ig))

#Entropy for Age Below 30 = false
p <- c(1/4,3/4)
e_false <- entropy(p)
#print(paste0("P(Recidivist|Age Below 30 = false) = ", 2/3))
#print(paste0("E(Recidivist|Age Below 30 = false) = ", e_false))

#Entropy for Age Bloew 30 = true
p <- c(1)
e_true <- entropy(p)
#print(paste0("P(Recidivist|Age Below 30 = true) = ", 1/3))
#print(paste0("E(Recidivist|Age Below 30 = true) = ", e_true))

#Information gain for feature Age Below 30
ig <- e_target - ((2/3*e_false)+(1/3*e_true))
print(paste0("Information gain for feature Age Below 30: ", ig))

#Entropy for drug dependent = false
p <- c(2/5,3/5)
e_false <- entropy(p)
#print(paste0("P(Recidivist|Drug Dependent = false) = ", 5/6))
#print(paste0("E(Recidivist|Drug Dependent = false) = ", e_false))

#Entropy for drug dependent = true
p <- c(1)
e_true <- entropy(p)
#print(paste0("P(Recidivist|Drug Dependent = true) = ", 1/6))
#print(paste0("E(Recidivist|Drug Dependent = false) = ", e_true))

#Information gain for feature Drug Dependent
ig <- e_target - ((5/6*e_false)+(1/6*e_true))
print(paste0("Information gain for feature Drug Dependent: ", ig))
```
    
    So our model will start with the feature Age Below 30 and will look like this to start
    
```{mermaid}
flowchart TD
  Age_Below_30 -- true --- True
  Age_Below_30 -- false --- Todo
```
    
    Next we calculate the information gain for each feature when Age Below 30 is false
    
```{r}
#Target entropy
p <- c(1/4,3/4)
e_target <- entropy(p)
print(paste0("Entropy(Recidivist|Age Below 30 = false) = ", e_target))

#Entropy for drug dependent = false
p <- c(1)
e_false <- entropy(p)
#print(paste0("P(Recidivist|Age Below 30 = false|Drug Dependent = false) = ", 3/4))
#print(paste0("E(Recidivist|Age Below 30 = false|Drug Dependent = false) = ", e_false))

#Entropy for drug dependent = true
p <- c(1)
e_true <- entropy(p)
#print(paste0("P(Recidivist|Age Below 30 = false|Drug Dependent = true) = ", 1/4))
#print(paste0("E(Recidivist|Age Below 30 = false|Drug Dependent = true) = ", e_true))

#Information gain for drug dependent
ig <- e_target - ((3/4*e_false)+(1/4*e_true))
print(paste0("Information gain for feature Drug Dependent: ", ig))

#Entropy for good behavior = false
p <- c(1)
e_false <- entropy(p)
#print(paste0("P(Recidivist|Age Below 30 = false|Good Behavior = false) = ", 1/4))
#print(paste0("E(Recidivist|Age Below 30 = false|Good Behavior = false) = ", e_false))

#Entropy for good behavior = true
p <- c(1/3,2/3)
e_true <- entropy(p)
#print(paste0("P(Recidivist|Age Below 30 = false|Good Behavior = true) = ", 3/4))
#print(paste0("E(Recidivist|Age Below 30 = false|Good Behavior = true) = ", e_true))

#Information gain for good behavior
ig <- e_target - ((1/4*e_false)+(3/4*e_true))
print(paste0("Information gain for feature Good Behavior: ", ig))
```    
    
    We select the feature Drug Dependent as our next node and our tree is updated as so
    
```{mermaid}
flowchart TD
  Age_Below_30 -- true --- True
  Age_Below_30 -- false --- Drug_Dependent
  Drug_Dependent -- true --- True
  Drug_Dependent -- false --- False
```
    
Worth noting since the information gain for splitting by Drug Dependent equaled the entropy of the set it resulted in a node with only pure sets
    
1. What prediction will the decision tree generated in part (a) of this question return for the following query?
    
    **Good Behavior** = false, **Age Below 30** = false, **Drug Dependent** = true
    
    Recidivist = True
    
1. What prediction will the decision tree generated in part (a) of this question return for the following query?

    **Good Behavior** = true, **Age Below 30** = true, **Drug Dependent** = false
    
    Recidivist = True
    
# Information Gain

The Table below lists a sample of data from a census.

```{r}
#| echo: false
c1 <- c(1,2,3,4,5,6,7,8)
c2 <- c(39, 50, 18, 28, 37, 24, 52, 40)
c3 <- c('bachelors', 'bachelors', 'high school', 'bachelors', 'high school', 'high school', 'high school', 'doctorate')
c4 <- c('never married', 'married', 'never married', 'married', 'married', 'never married', 'divorced', 'married')
c5 <- c('transport', 'professional', 'agriculture', 'professional', 'agriculture', 'armed forces', 'transport', 'professional')
c6 <- c('25K-50K', '25K-50K', 'below 25K', '25K-50K', '25K-50K', 'below 25K', '25K-50K', 'over 50K')
df2 <- data.frame(c1,c2,c3,c4,c5,c6)
# print(df2)
```


```{r}
#| echo: false
#| tbl-cap-location: margin
knitr::kable(df2, caption = 'Census data.', col.names = c('ID', 'Age', 'Education', 'Marital Status', 'Occupation', 'Annual Income'), align = "lrrrrr")
```


There are four descriptive features and one target feature in this dataset:

- **Age**, a continuous feature listing the age of the individual.
- **Education**, a categorical feature listing the highest education award achieved by the individual (high school, bachelors, doctorate).
- **Marital Status** (never married, married, divorced).
- **Occupation** (transport = works in the transportation industry; professional = doctors, lawyers, etc.; agriculture = works in the agricultural industry; armed forces = is a member of the armed forces).
- **Annual Income**, the target feature with 3 levels (<25K, 25K-50K, >50K).


1. Calculate the **entropy** for this dataset.
    
    The entropy is calculated for the target feature of the dataset.
    
```{r}
p <- c(5/8, 2/8, 1/8)
e_target <- entropy(p)
print(paste0("Dataset entropy = ",e_target))
```

1. Calculate the **Gini** index for this dataset.
    
    Gini index formula -> G = 1 - sum(p_i)^2
    
```{r}
#gini index for the dataset
g_target <- 1 - ((2/8)**2 + (5/8)**2 + (1/8)**2)
print(paste0("Dataset gini index = ", g_target))
```

1. When building a decision tree, the easiest way to handle a continuous feature is to define a threshold around which splits will be made. What would be the optimal threshold to split the continuous **Age** feature (use information gain based on entropy as the feature selection measure)?
    
    Lets compare being lazy and splitting it in half versus using a range of 10 years
    
```{r}
#splitting in half

#age < 39
p <- c(1/2,1/2)
p1 <- 1/2
e1 <- entropy(p)

#age > 39
p <- c(3/4,1/4)
p2 <- 1/2
e2 <- entropy(p)

ig <- e_target - ((p1*e1)+(p2*e2))
print(paste0("Information gain from split in half: ",ig))

#Splitting by 10

#age < 20
p <- c(1)
p2 <- 1/8
e2 <- entropy(p)

#age < 30
p <- c(1/2,1/2)
p3 <- 2/8
e3 <- entropy(p)

#age < 40
p <- c(1)
p4 <- 2/8
e4 <- entropy(p)

#age < 50
p <- c(1)
p5 <- 1/8
e5 <- entropy(p)

#age 60
p <- c(1)
p6 <- 2/8
e6 <- entropy(p)

ig <- e_target - ((p2*e2)+(p3*e3)+(p4*e4)+(p5*e5)+(p6*e6))
print(paste0("Information gain from splitting by 10: ",ig))
```

    Clearly higher seperation leads to greater information gain

1. Calculate information gain (based on entropy) for the **Education**, **Marital Status**, and **Occupation** features.
    
    Your answer goes here.
```{r}
#Education

#bachelors and doctorate can be ignored for entropy 0

#high-school
p <- c(1/2,1/2)
p_hs <- 1/2
e_hs <- entropy(p)

ig_ed <- e_target - (p_hs*e_hs)
print(paste0("Information gain for education feature: ", ig_ed))

#Marital Status

#divorced can be ignored for entropy 0

#married
p <- c(3/4,1/4)
p_m <- 1/2
e_m <- entropy(p)

#never-married
p <- c(1/3,2/3)
p_nm <- 3/8
e_nm <- entropy(p)

ig_ms <- e_target - ((p_m*e_m) + (p_nm*e_nm))
print(paste0("Information gain for marital status feature: ", ig_ms))

#Occupation

#transport, and armed forces can ge ignored for entropy 0

#professional
p <- c(2/3,1/3)
p_p <- 3/8
e_p <- entropy(p)

#agriculture
p <- c(1/2,1/2)
p_ag <- 1/4
e_ag <- entropy(p)

ig_oc <- e_target - ((p_p*e_p) + (p_ag*e_ag))
print(paste0("Information gain for occupation feature: ", ig_oc))
```

1. Calculate the information gain ratio (based on entropy) for **Education**, **Marital Status**, and **Occupation** features.
    
    Your answer goes here.
```{r}
#Education entropy
e_ed <- entropy(3/8) + entropy(4/8) + entropy(1/8)
igr_ed <- ig_ed / e_ed
print(paste0("Information gain ratio for Education: ", igr_ed))

#Marital Status entropy
e_ms <- entropy(3/8) + entropy(4/8) + entropy(1/8)
igr_ms <- ig_ms / e_ms
print(paste0("Information gain ratio for Marital Status: ", igr_ms))

#Occupation entropy
e_oc <- entropy(2/8) + entropy(2/8) + entropy(3/8) + entropy(1/8)
igr_oc <- ig_oc / e_oc
print(paste0("Information gain ratio for Occupation: ", igr_oc))
```
    
1. Calculate information gain using the Gini index for the **Education**, **Marital Status**, and **Occupation** features.
    
```{r}
#gini index for education

#education = bachelors and education = doctorate can be ignored as their gini index values will be zero

#education = high school
p_high_school <- 4/8
g_high_school <- 1 - ((1/2)**2 + (1/2)**2)


G_Education <- p_high_school * g_high_school
ig <- g_target - G_Education
print(paste0("Information gain for Education: ", ig))

#gini index for marital status

#status = married
p_married <- 4/8
g_married <- 1 - ((3/4)**2 + (1/4)**2)

#status = never
p_never <- 3/8
g_never <- 1 - ((1/3)**2 + (2/3)**2)

#status = divorced can be ignored since its gini index value is zero

G_marital_status <- (p_married * g_married) + (p_never * g_never)
ig <- g_target - G_marital_status
print(paste0("Information gain for Marital Status: ", ig))

#Gini Index for occupation

#occupation = transport and armed forces can be ignored for zero value

#occupation = professional
p_pro <- 3/8
g_pro <- 1 - ((2/3)**2 + (1/3)**2)

#occupation = agriculture
p_ag <- 2/8
g_ag <- 0.5

G_Occupation <- (p_pro*g_pro) + (p_ag*g_ag)
ig <- g_target - G_Occupation
print(paste0("Information gain for Occupation: ", ig))
```

    
# Decision Tree Error Pruning

Shown in the figure below shows a decision tree for predicting heart disease. The descriptive features in this domain describe whether the patient suffers from chest pain (**Chest Pain**) and blood pressure (**Blood Pressure**). The binary target feature is **Heart Disease**. The table below the diagram lists a pruning set from this domain.

<!--

heart.dot file

digraph G {
   node [style=filled, color=lightblue]
   edge [style=solid, color=blue]


   A [label = "Chest Pain [true]"]
   B [label = "Blood Pressure [false]"]
   C [shape=box, label = "true"]
   D [shape=box, label = "true"]
   E [shape=box, label = "false"]

   A -> B [taillabel="false", labeldistance=5.5]
   A -> C [taillabel="true", labeldistance=2.5]
   B -> D [taillabel="high", labeldistance=3.5, labelangle=-30]
   B -> E [taillabel="low", labeldistance=3.5, labelangle=30]

}

Command to produce heart.png file

> dot -Tpng heart.dot -o heart.png
-->



![](./heart.png)





```{r}
#| echo: false
c1 <- c(1,2,3,4,5)
c2 <- c('false', 'true', 'false', 'true', 'false')
c3 <- c('high', 'low', 'low', 'high', 'high')
c4 <- c('false', 'true', 'false', 'true', 'false')
df11 <- data.frame(c1,c2,c3,c4)
# print(df11)
```


```{r}
#| echo: false
#| tbl-cap-location: margin
knitr::kable(df11, caption = 'Pruning set.', col.names = c('ID', 'Chest Pain', 'Blood Pressure', 'Heart Disease'), align = "lrrr")
```



Using the pruning set, apply reduced error pruning to the decision tree. Assume that the algorithm is applied in a bottom-up, left-to-right fashion. For each iteration of the algorithm, indicate the subtrees considered as pruning candidates, explain why the algorithm chooses to prune or leave these subtrees in the tree, and illustrate the tree that results from each iteration.


The first subtree considered is under the blood pressure node

```{mermaid}
flowchart TD
  BloodPressure_false -- high --- true
  BloodPressure_false -- low --- false
```

False is the value returned from the root node and results in zero errors in the pruning set, the first leaf node results in 0 errors also but the true leaf node results in 2 errors. Since the leaf nodes result in more errors than the root node they are pruned from the tree leaving the following

```{mermaid}
flowchart TD
  ChestPain_true -- true --- true
  ChestPain_true -- false --- false
```

True is the value returned from the root node which results in 3 errors, the first and second leaf nodes result in 0 errors, therefore this tree will not be pruned and will be the final tree

# Random Forest

The Table below lists a dataset containing the details of five participants in a heart disease study. The target feature *Risk* describes their risk of heart disease. Each patient is described in terms of four binary descriptive features:

- **Exercise** - how regularly do they exercise
- **Smoker** - do they smoke
- **Obese** - are they overweight
- **Family** - did any of their parents or siblings suffer from heart disease

```{r}
#| echo: false
ID <- c(1,2,3,4,5)
Exercise <- c('daily', 'weekly', 'daily', 'rarely', 'rarely')
Smoker <- c('false', 'true', 'false', 'true', 'true')
Obese <- c('false', 'false', 'false', 'true', 'true')
Family <- c('yes', 'yes', 'no', 'yes', 'no')
Risk <- c('low', 'high', 'low', 'high', 'high')
df3 <- data.frame(ID, Exercise, Smoker, Obese, Family, Risk)
# print(df3)
```
```{r}
#| echo: false
#| tbl-cap-location: margin
knitr::kable(df3, caption = 'Heart disease study dataset.', align = "lrrrrr")
```


```{r}
#| echo: false
#| # Bootstrap Sample A
ID <- c(1,2,2,5,5)
Exercise <- c('daily', 'weekly', 'weekly', 'rarely', 'rarely')
Family <- c('yes', 'yes', 'yes', 'yes', 'no')
Risk <- c('low', 'high', 'high', 'high', 'high')
df4 <- data.frame(ID, Exercise, Family, Risk)
# print(df4)
```


```{r}
#| echo: false
#| # Bootstrap Sample B
ID <- c(1,2,2,4,5)
Smoker <- c('false', 'true', 'true', 'true', 'true')
Obese <- c('false', 'false', 'false', 'true', 'true')
Risk <- c('low', 'high', 'high', 'high', 'high')
df5 <- data.frame(ID, Smoker, Obese, Risk)
# print(df5)
```

```{r}
#| echo: false
#| # Bootstrap Sample C
ID <- c(1,1,2,4,5)
Obese <- c('false', 'false', 'false', 'true', 'true')
Family <- c('yes', 'yes', 'yes', 'yes', 'no')
Risk <- c('low', 'low', 'high', 'high', 'high')
df6 <- data.frame(ID, Obese, Family, Risk)
# print(df6)
```

As part of the study researchers have decided to create a predictive model to screen participants based on their risk of heart disease. You have been asked to implement this screening model using a random forest. Table below list three bootstrap samples that have been generated from the above dataset.

```{r}
#| echo: false
#| tbl-cap-location: margin
knitr::kable(df4, caption = 'Heart disease study dataset bootstrap sample A.', align = "lrrrr")
```

```{r}
#| echo: false
#| tbl-cap-location: margin
knitr::kable(df5, caption = 'Heart disease study dataset bootstrap sample B.', align = "lrrrr")
```
    
```{r}
#| echo: false
#| tbl-cap-location: margin
knitr::kable(df6, caption = 'Heart disease study dataset bootstrap sample C.', align = "lrrrr")
```
    
    
1. Using these bootstrap samples create the decision trees that will be in the random forest model (use entropy based information gain as the feature selection criterion).

First we should see which feature in each sample gives us the greatest information gain starting with A

```{r}
#Entropy for A
e_A <- entropy(1/5) + entropy(4/5)
print(paste0("Entropy of A: ", e_A))

#information gain for features in A

#Exercise

#daily
p <- c(1)
p_d <- 1/5
e_d <- entropy(p)

#weekly
p <- c(1)
p_w <- 2/5
e_w <- entropy(p)

#rarely
p <- c(1)
p_r <- 2/5
e_r <- entropy(p)

ig_ex <- e_A - ((p_d*e_d) + (p_w*e_w) + (p_r*e_r))
print(paste0("Information gain for Exercise: ", ig_ex))

#Family

#yes
p <- c(3/4,1/4)
p_y <- 3/5
e_y <- entropy(p)

#no
p <- c(1)
p_n <- 2/5
e_n <- entropy(p)

ig_f <- e_A - ((p_y*e_y) + (p_n*e_n))
print(paste0("Information gain for Family: ", ig_f))
```
    
We can see the exercise has the highest information gain, actually splitting on exercise generates pure sets so after inserting it at the root node we will be done creating the tree for A
    
```{mermaid}
flowchart TD
  Exercise -- daily --- low
  Exercise -- weekly --- high
  Exercise -- rarely --- high
```

Now we can do the same for B, luckily B has the same distribution of target features as A so its entropy is the same

```{r}
e_B <- e_A
print(paste0("Entropy for B: ", e_B))

#Information gain for smoker

#true
p <- c(1)
p_st <- 4/5
e_st <- entropy(p)

#false
p <- c(1)
p_sf <- 1/5
e_sf <- entropy(p)

ig_s <- e_B - ((p_st*e_st) + (p_sf*e_sf))
print(paste0("Information gain for Smoker: ", ig_s))

#Information gain for obese

#true
p <- c(1)
p_ot <- 2/5
e_ot <- entropy(p)

#false 
p <- c(2/3,1/3)
p_of <- 3/5
e_of <- entropy(p)

ig_o <- e_B - ((p_ot*e_ot) + (p_of*e_of))
print(paste0("Information gain for Obese: ", ig_o))
```

We can see again the one of the features, smoker, splits the set into pure sets so our resulting tree will be

```{mermaid}
flowchart TD 
  Smoker -- true --- high
  Smoker -- false --- low
```

Lastly we apply the same method to C

```{r}
e_C <- entropy(2/5) + entropy(3/5)
print(paste0("Entropy for C is: ", e_C))

#information gain for obese

#true
p <- c(1)
p_ot <- 2/5
e_ot <- entropy(p)

#false
p <- c(2/3,1/3)
p_of <- 3/5
e_of <- entropy(p)

ig_o <- e_C - ((p_ot*e_ot) + (p_of*e_of))
print(paste0("Information gain for Obese: ", ig_o))

#family

#yes
p <- c(1/2,1/2)
p_fy <- 4/5
e_fy <- entropy(p)

#no
p <- c(1)
p_fn <- 1/5
e_fn <- entropy(p)

ig_f <- e_C - ((p_fy*e_fy) + (p_fn*e_fn))
print(paste0("information gain for Family: ", ig_f))
```

We can see we should start with Obese which leaves us with a pure set and an impure set, but rather than creating more nodes for our impure set since we only have one feature left (Family) and all instances in this split have the same level for this feature, we simply create a leaf node with the majority target within.

```{mermaid}
flowchart TD
  Obese -- true --- high
  Obese -- false --- low
```

And now we're done

1. Assuming the random forest model you have created uses majority voting, what prediction will it return for the following query:

    Exercise=rarely, Smoker=false, Obese=true, Family=yes
    
    Tree 1 : Exercise=rarely --> risk=high
    Tree 2 : Smoker=false --> risk=low
    Tree 3 : Obese=true --> risk=high
    
    Will return high risk
    