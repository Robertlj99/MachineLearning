---
title: "Written Assignment 06"
author: "Robert Johnson"
date: "`r format(Sys.time(), '%d %B %Y')`"
number-sections: true
number-depth: 3
format:
  html:
    toc: true
    toc-location: right
    number-sections: true
    number-depth: 3
    html-math-method: katex
    embed-resources: true
    self-contained: true
    code-fold: true
# bibliography: dasc-6000.bib 
# csl: ieee-with-url.csl
# linkcolor: red
# urlcolor: blue
# link-citations: yes
# header-includes:
#   - \usepackage[ruled,vlined,linesnumbered]{algorithm2e}
---

```{r setup, include=FALSE} 
knitr::opts_chunk$set(warning = FALSE, message = FALSE) 
```

# Assignment Goal {.unnumbered}

The overarching goal for this assignment is to assess your understanding of the probability-based machine learning algorithms. 

# Instructions {.unnumbered}

Please show all your work. Simply providing the final answer is treated as no response. If you do not use R or Python notebooks, it is fine. However, the document structure should be preserved if you choose to use Microsoft Word or something else.

Please submit your response either as a self-contained HTML or PDF document.



# Computing Probability - Equally Likely Outcomes

(a) Three people flip a fair coin. What is the probability that exactly two of them will get heads?

    There are 8 possible outcomes and exactly 3 of them have two people getting heads thus the probability of this is 3/8 = 0.375


(a) Twenty people flip a fair coin. What is the probability that exactly eight of them will get heads?


    This can be computed using the binomial distrubution as follows
$$
P(8) = {20 \choose 8}* 0.5^8 * (1-0.5)^{20-8} = 0.1201
$$


(c) Twenty people flip a fair coin. What is the probability that at least four of them will get heads?



The probability for this is equal to 1 minus the probability that less than 4 people get heads. To find the probability of less than 4 people getting heads we can sum up the probabilties of exactly 3 people, exactly 2 people, and exactly 1 person getting heads using the following binomial distrubutions
    
$$
P(1) = {20 \choose 1} * 0.5^1 * (1-0.5)^{20-1} = 0.0000190735
$$
$$
P(2) = {20 \choose 2} * 0.5^2 * (1-0.5)^{20-2} = 0.000181198
$$
$$
P(3) = {20 \choose 3} * 0.5^2 * (1-0.5)^{20-3} = 0.001087189
$$
Summing these three gives us the probability of 3 or less people getting heads

$$
\sum = 0.001087189 + 0.000181198 + 0.0000190735 = 0.0012874605
$$
Finally we subtract this from 1 to find the probability of at least 4 people getting heads

$$
P = 1 - 0.0012874605 = 0.9987125
$$

# Estimating Probabilities

The table gives details of symptoms that patients presented and whether they were suffering from meningitis.

```{r}
#| echo: false
library(readr)
df1 <- readr::read_csv("./meningitis 3.csv", col_names = FALSE)
column_names <- c("ID", "Headache", "Fever", "Vomiting", "Meningitis")
colnames(df1) <- column_names
```


```{r}
#| echo: false
#| tbl-cap-location: margin
knitr::kable(df1, caption = 'Data for a predicting Meningitis.', align = "lrrrr")
```


Using this dataset calculate the following probabilities:


(a) P(Vomiting = true)

    6/10 = 0.6


(a) P(Headache = false)

    3/10 = 0.3


(a) P(Headache = true, Vomiting = false)

    1/10 = 0.1


(a) P(Vomiting = false $\mid$ Headache = true)

    1/7 = 0.1429


(a) P(Meningitis $\mid$ Fever = true, Vomiting = false)

    P(Meningitis = true | Fever = true, Vomitting = false) = 1/4 = 0.25
    
    P(Meningitis = false | Fever = true, Vomitting = false) = 3/4 = 0.75
    
    <0.25,0.75>



# Naive Bayes Model

Predictive data analytics models are often used as tools for process quality control and fault detection. The task in this question is to create a naive Bayes model to monitor a waste water treatment plant. The table below lists a dataset containing details of activities at a waste water treatment plant for 14 days. Each day is described in terms of six descriptive features that are generated from different sensors at the plant. SS-IN measures the solids coming into the plant per day; SED-IN measures the sediment coming into the plant per day; COND-IN measures the electrical conductivity of the water coming into the plant. The features SS-OUT, SED-OUT, and COND-OUT are the corresponding measurements for the water flowing out of the plant. The target feature, STATUS, reports the current situation at the plant: ok, everything is working correctly; settler, there is a problem with the plant settler equipment; or solids, there is a problem with the amount of solids going through the plant.


```{r}
#| echo: false
library(readr)
df2 <- readr::read_csv("./naive-bayes.csv", col_names = FALSE)
column_names <- c("ID", "SS-IN", "SED-IN", "COND-IN", "SS-OUT", "SED-OUT", "COND-OUT", "STATUS")
colnames(df2) <- column_names
```


```{r}
#| echo: false
#| tbl-cap-location: margin
knitr::kable(df2, caption = 'Data for a naive Bayes model.', align = "lrrrrrrr")
```


(a) Create a naive Bayes model that uses probability density functions to model the descriptive features in this dataset (assume that all the descriptive features are normally distributed).

    Priority Probabilities:
    
    P(Status = ok) = 4/13 = 0.3077
    
    P(Status = settler) = 5/13 = 0.3864
    
    P(Status = solids) = 4/13 = 0.3077
    
    The rest of the work involves taking the mean and standard deviation for each feature given our priority target for one example P(Ss-in | ok) has a mean of 189 and standard deviation of 45.4 while P(Ss-in | settler) has a mean of 200.8 and standard deviation of 55.13


(a) What prediction will the naive Bayes model return for the following query? SS-IN = 222, SED-IN = 4.5, COND-IN = 1,518, SS-OUT = 74 SED-OUT = 0.25, COND-OUT = 1,642

    The model will predict status equals settler


# Transforming Continuous Features to Descriptive Features

The table below lists a dataset containing details of policy holders at an insurance company. The descriptive features included in the table describe each policy holders' ID, occupation, gender, age, the type of insurance policy they hold, and their preferred contact channel. The preferred contact channel is the target feature in this domain.

```{r}
#| echo: false
library(readr)
df3 <- readr::read_csv("./insurance-policy 2.csv", col_names = FALSE)
column_names <- c("ID", "Occupation", "Gender", "Age", "Policy Type", "Pref Channel")
colnames(df3) <- column_names
```


```{r}
#| echo: false
#| tbl-cap-location: margin
knitr::kable(df3, caption = 'Data for a insurance policy model.', align = "llllll")
```


(a) Using equal-frequency binning transform the Age feature into a categorical feature with three levels: young, middle-aged, mature.

    Young <= 21 
    
    21 < Middle-aged < 50
    
    Mature > 50
    
    

(a) Examine the descriptive features in the dataset and list the features that you would exclude before you would use the dataset to build a predictive model. For each feature you decide to exclude, explain why you have made this decision.

    You may remove the ID column as it will make no impact on the target feature
    
    I am also choosing to remove the occupation column as it is very random and likely wont impact the target feature while significantly impacting how much I need to calculate in the next problem

(a) Calculate the probabilities required by a naive Bayes model to represent this domain.

    P(PrefChannel | gender):
    P(email | female) = 1/4 = 0.25
    P(phone | female) = 3/4 = 0.75
    P(email | male) = 3/5 = 0.6
    P(phone | male) = 2/5 = 0.4
    
    P(PrefChannel | age):
    P(email | young) = 2/3 = 0.66
    P(phone | young) = 1/3 = 0.33
    P(email | middle-aged) = 1/3 = 0.33
    P(phone | middle-aged) = 2/3 = 0.66
    P(email | mature) = 1/3 = 0.33
    P(phone | mature) = 2/3 = 0.66
    
    P(PrefChannel | PolicyType):
    P(email | planA) = 2/3 = 0.66
    P(phone | planA) = 1/3 = 0.33
    P(email | planB) = 1/2 = 0.5
    P(phone | planB) = 1/2 = 0.5
    P(email | planC) = 1/4 = 0.25
    P(phone | planC) = 3/4 = 0.75

(a) What target level will a naive Bayes model predict for the following query: Gender = female, Age = 30, Policy = planA.

    The female and middle-aged classifications will outweight the policy type and the model will predict phone for the target feature



# Another Naive Bayes Model

Imagine that you have been given a dataset of 1,000 documents that have been classified as being about entertainment or education. There are 700 entertainment documents in the dataset and 300 education documents in the dataset. The tables below give the number of documents from each topic that a selection of words occurred in.

```{r}
#| echo: false
library(readr)
df4 <- readr::read_csv("./entertainment.csv", col_names = FALSE)
```


```{r}
#| echo: false
#| tbl-cap-location: margin
knitr::kable(df4, col.names = NULL, caption = 'Word-document counts for the entertainment dataset.', align = "rrrrrr")
```

```{r}
#| echo: false
library(readr)
df5 <- readr::read_csv("./education.csv", col_names = FALSE)
```


```{r}
#| echo: false
#| tbl-cap-location: margin
knitr::kable(df5, col.names = NULL, caption = 'Word-document counts for the education dataset.', align = "rrrrrr")
```


(a) What target level will a naive Bayes model predict for the following query document: machine learning is fun

    First I calculated the word probabilites for each class (not shown) after that I mulitiplied the probabilites of each word in the query for each individual class (also not shown). The education class returned a higher score and is therefore what would be predicted

(a) What target level will a naive Bayes model predict for the following query document: christmas family fun

    Reproducing the same method as the one above for this query results in identical probabilties for each class

(a) What target level will a naive Bayes model predict for the query document in part (b) of this question, if Laplace smoothing with k = 10 and a vocabulary size of 6 is used?

    Implementing the following changes in our calculation results in the entertainment class being selected






